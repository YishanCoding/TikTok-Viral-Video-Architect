import { GoogleGenAI, Type, Schema } from "@google/genai";
import { Language, Duration, ScriptRow, GeneratedContent, ScriptVariant, VideoScene } from "../types";

// --- Helper: Convert File to Base64 (Gemini Part) ---
export const fileToGenerativePart = async (file: File): Promise<{ inlineData: { mimeType: string; data: string } }> => {
  return new Promise((resolve, reject) => {
    const reader = new FileReader();
    reader.onloadend = () => {
      const base64String = reader.result as string;
      const base64Data = base64String.split(',')[1];
      resolve({
        inlineData: {
          mimeType: file.type,
          data: base64Data
        }
      });
    };
    reader.onerror = reject;
    reader.readAsDataURL(file);
  });
};

// --- Helper: Capture Video Frames ---
export const captureVideoFrames = async (videoFile: File, timestamps: { start: number }[]): Promise<string[]> => {
  return new Promise((resolve, reject) => {
    const video = document.createElement('video');
    video.preload = 'metadata';
    video.src = URL.createObjectURL(videoFile);
    video.muted = true;
    video.playsInline = true;

    const canvas = document.createElement('canvas');
    const ctx = canvas.getContext('2d');
    const screenshots: string[] = [];
    let currentIndex = 0;

    video.onloadedmetadata = () => {
      canvas.width = video.videoWidth / 2; // Reduce resolution for performance
      canvas.height = video.videoHeight / 2;
      
      const captureNext = () => {
        if (currentIndex >= timestamps.length) {
          URL.revokeObjectURL(video.src);
          resolve(screenshots);
          return;
        }

        video.currentTime = timestamps[currentIndex].start;
      };

      video.onseeked = () => {
        if (ctx) {
            ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
            screenshots.push(canvas.toDataURL('image/jpeg', 0.7)); // Compress
        }
        currentIndex++;
        captureNext();
      };

      captureNext(); // Start loop
    };

    video.onerror = () => {
      reject(new Error("Failed to load video for frame capture"));
    };
  });
};

// --- API: Generate White Background Product Grid (Step 1) ---
export const generateProductGrid = async (imageParts: any[], description: string): Promise<string> => {
  try {
    const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });

    const prompt = `
      Create a "9-grid reference sheet" (3x3 grid layout) for this product on a pure WHITE BACKGROUND.
      The product is described as: "${description}".
      The grid should contain 9 distinct panels showing:
      - Front view
      - Back view
      - Side profiles
      - Top/down view
      - Close-ups of material textures
      - Detail shots of features
      Style: Professional E-commerce Product Photography, Studio Lighting, Clean, High Resolution. 
      No text overlays, No props, just the product.
    `;

    const response = await ai.models.generateContent({
      model: 'gemini-3-pro-image-preview', // High quality for reference
      contents: {
        parts: [
          ...imageParts,
          { text: prompt }
        ]
      },
      config: {
        imageConfig: {
          aspectRatio: "9:16",
          imageSize: "4K"
        }
      }
    });

    for (const part of response.candidates?.[0]?.content?.parts || []) {
      if (part.inlineData) {
        return part.inlineData.data;
      }
    }
    throw new Error("No image generated by the model.");
  } catch (error) {
    console.error("Error generating product grid:", error);
    throw error;
  }
};

// --- API: Analyze Video Content ---
export const analyzeVideoContent = async (videoFile: File): Promise<Omit<VideoScene, 'screenshot'>[]> => {
  try {
    const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });
    const videoPart = await fileToGenerativePart(videoFile);

    // Added stronger instructions for audio transcription
    const prompt = `
      Analyze this TikTok video in detail. Focus on both VISUALS and AUDIO.
      
      CRITICAL INSTRUCTION: You MUST LISTEN to the audio track and provide a word-for-word transcript.
      
      Break it down into scenes/shots. For each scene, provide:
      1. start_time (seconds, float)
      2. end_time (seconds, float)
      3. category (Choose from: 'Product Info', 'Usage', 'Benefits', 'Hook', 'CTA', 'Other')
      4. description (Visual description)
      5. transcript_original (Exact spoken words in original language. If silence/music, say "[Music]")
      6. transcript_translation (Translate spoken words to Simplified Chinese)

      Return strictly JSON.
    `;

    const responseSchema: Schema = {
      type: Type.OBJECT,
      properties: {
        scenes: {
          type: Type.ARRAY,
          items: {
            type: Type.OBJECT,
            properties: {
              start_time: { type: Type.NUMBER },
              end_time: { type: Type.NUMBER },
              category: { type: Type.STRING, enum: ['Product Info', 'Usage', 'Benefits', 'Hook', 'CTA', 'Other'] },
              description: { type: Type.STRING },
              transcript_original: { type: Type.STRING },
              transcript_translation: { type: Type.STRING },
            },
            required: ["start_time", "end_time", "category", "description", "transcript_original", "transcript_translation"]
          }
        }
      },
      required: ["scenes"]
    };

    const response = await ai.models.generateContent({
      model: 'gemini-3-flash-preview',
      contents: { parts: [videoPart, { text: prompt }] },
      config: {
        systemInstruction: "You are a video analysis expert capable of hearing audio and seeing visuals.",
        responseMimeType: "application/json",
        responseSchema: responseSchema
      }
    });

    const data = JSON.parse(response.text || "{}");
    
    // Format times to "MM:SS" string for UI
    const formatTime = (s: number) => {
        const mins = Math.floor(s / 60);
        const secs = Math.floor(s % 60);
        return `${mins.toString().padStart(2, '0')}:${secs.toString().padStart(2, '0')}`;
    };

    return data.scenes.map((s: any) => ({
      startTime: formatTime(s.start_time),
      endTime: formatTime(s.end_time),
      category: s.category,
      description: s.description,
      transcriptOriginal: s.transcript_original,
      transcriptTranslation: s.transcript_translation,
      rawStartTime: s.start_time // Keep raw for seeking
    }));

  } catch (error) {
    console.error("Analysis failed:", error);
    throw error;
  }
};

// --- API: Generate Single Scene Image (Nano Banana) with Reference ---
export const generateSceneImage = async (visualDescription: string, referenceImageBase64: string | null): Promise<string> => {
    const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });
    
    const parts: any[] = [];
    if (referenceImageBase64) {
      parts.push({
        inlineData: {
          mimeType: 'image/jpeg',
          data: referenceImageBase64
        }
      });
    }

    // UPDATED PROMPT: Authenticity, Handheld, Messy, US Home
    const prompt = `
      Generate a single photorealistic 9:16 vertical TikTok video frame.
      
      Scene Description: ${visualDescription}
      
      IMPORTANT STYLE CONSTRAINTS (Strict Adherence):
      1. Setting: Real, lived-in American home (e.g., slightly messy bathroom counter, cluttered kitchen table, bedroom with clothes). NOT a studio.
      2. Camera: Handheld iPhone aesthetic. Slightly grainy, imperfect lighting, maybe a bit blurry or motion blur to suggest movement.
      3. Vibe: Amateur UGC (User Generated Content), authentic, relatable. NOT commercial/ad-like.
      
      PRODUCT INTEGRATION:
      If the scene description mentions the product, it must look exactly like the reference image provided.
      If the scene description is a general situation (e.g. "dirty laundry pile"), show that situation authentically without forced product placement unless specified.
    `;

    parts.push({ text: prompt });

    const response = await ai.models.generateContent({
        model: 'gemini-2.5-flash-image', // Nano Banana
        contents: { parts: parts },
        config: {}
    });

    for (const part of response.candidates?.[0]?.content?.parts || []) {
        if (part.inlineData) {
            return part.inlineData.data;
        }
    }
    throw new Error("No image generated");
};

// --- API: Regenerate Single Script Row ---
export const regenerateScriptRow = async (
    originalRow: ScriptRow, 
    context: string, 
    language: string
): Promise<ScriptRow> => {
    const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });
    
    const prompt = `
        Regenerate this specific script row to be more engaging for a US TikTok audience.
        Context of video: ${context}
        
        Current Row:
        Time: ${originalRow.timeframe}
        Visual: ${originalRow.visual}
        Audio: ${originalRow.audio}
        
        Task: 
        1. Rewrite the Visual and Audio (in ${language}).
        2. Provide Simplified Chinese translations for both.
        3. Visuals must be "show, don't tell". If audio describes a problem, visual shows the problem, not the product.
    `;

    const responseSchema: Schema = {
         type: Type.OBJECT,
         properties: {
             visual: { type: Type.STRING },
             visual_translation: { type: Type.STRING },
             audio: { type: Type.STRING },
             audio_translation: { type: Type.STRING },
             style: { type: Type.STRING }
         }
    };

    const response = await ai.models.generateContent({
        model: 'gemini-3-flash-preview',
        contents: { parts: [{ text: prompt }] },
        config: { responseMimeType: "application/json", responseSchema }
    });

    const data = JSON.parse(response.text || "{}");
    return {
        ...originalRow,
        visual: data.visual,
        visualTranslation: data.visual_translation,
        audio: data.audio,
        audioTranslation: data.audio_translation,
        style: data.style
    };
};

/**
 * Step 2: Generate the Script and Sora Prompt (Updated to use Analysis)
 */
export const generateScriptAndPrompt = async (
  imageParts: any[],
  analysisData: VideoScene[] | null,
  description: string, 
  language: Language, 
  duration: Duration,
  count: number = 1
): Promise<ScriptVariant[]> => {
  try {
    const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });
    const durationSec = duration === Duration.SHORT ? 15 : 25;
    
    let analysisContext = "";
    if (analysisData) {
        analysisContext = `
        REFERENCE VIDEO ANALYSIS (Use this structure as a proven viral template):
        ${analysisData.map(s => `[${s.startTime}-${s.endTime}] (${s.category}):
           Visual: ${s.description}
           Audio: ${s.transcriptOriginal}
        `).join('\n')}
        `;
    }

    // UPDATED SYSTEM INSTRUCTION: US UGC Authenticity & Audio-Visual Sync
    const systemInstruction = `
      You are a world-class TikTok E-commerce Short Video Expert targeting the US Market.
      Your goal is to create ${count} DISTINCT high-converting UGC video scripts.
      
      CRITICAL STYLE GUIDELINES (US UGC):
      1. Setting: Authentic US homes (messy, lived-in). Avoid sterile studios.
      2. Vibe: Handheld, amateur, relatable, "Lazy Girl Hack" style.
      3. Visual Strategy: STRICT AUDIO-VISUAL ALIGNMENT.
         - If audio discusses a "pain point" (e.g., "I hate washing underwear"), the Visual MUST show the action of washing underwear or a pile of dirty clothes. DO NOT show the product yet.
         - Only show the product when the audio introduces the solution.
      
      CONSTRAINTS:
      1. Language: STRICTLY output the script audio in ${language}.
      2. Duration: Each script must fit exactly into ${durationSec} seconds.
      3. Translations: Provide Simplified Chinese translations for Visuals, Audio, and the Sora Prompt.
    `;

    let userPrompt = `
      Product Description: ${description}
      ${analysisContext}
      
      Task 1: Create ${count} different TikTok Scripts.
      Task 2: Create a corresponding Sora-2 Prompt for each script.
    `;

    // UPDATED SCHEMA: Include translations
    const responseSchema: Schema = {
      type: Type.OBJECT,
      properties: {
        variants: {
          type: Type.ARRAY,
          items: {
            type: Type.OBJECT,
            properties: {
              name: { type: Type.STRING },
              script: {
                type: Type.ARRAY,
                items: {
                  type: Type.OBJECT,
                  properties: {
                    timeframe: { type: Type.STRING },
                    visual: { type: Type.STRING, description: "Description of the scene in English" },
                    visual_translation: { type: Type.STRING, description: "Simplified Chinese translation of visual" },
                    audio: { type: Type.STRING, description: "Spoken audio in target language" },
                    audio_translation: { type: Type.STRING, description: "Simplified Chinese translation of audio" },
                    style: { type: Type.STRING }
                  },
                  required: ["timeframe", "visual", "visual_translation", "audio", "audio_translation", "style"]
                }
              },
              soraPrompt: { type: Type.STRING, description: "English prompt" },
              sora_prompt_translation: { type: Type.STRING, description: "Simplified Chinese translation of prompt" }
            },
            required: ["name", "script", "soraPrompt", "sora_prompt_translation"]
          }
        }
      },
      required: ["variants"]
    };

    const parts = [...imageParts, { text: userPrompt }];

    const response = await ai.models.generateContent({
      model: 'gemini-3-flash-preview',
      contents: { parts },
      config: {
        systemInstruction,
        responseMimeType: "application/json",
        responseSchema
      }
    });

    const jsonText = response.text;
    const parsed = JSON.parse(jsonText);
    
    // Map response to internal types
    return parsed.variants.map((v: any) => ({
      id: crypto.randomUUID(),
      name: v.name,
      soraPrompt: v.soraPrompt,
      soraPromptTranslation: v.sora_prompt_translation,
      script: v.script.map((s: any) => ({
        id: crypto.randomUUID(),
        timeframe: s.timeframe,
        visual: s.visual,
        visualTranslation: s.visual_translation,
        audio: s.audio,
        audioTranslation: s.audio_translation,
        style: s.style
      }))
    }));

  } catch (error) {
    console.error("Error generating scripts:", error);
    throw error;
  }
};

/**
 * Orchestrator function 
 */
export const generateCampaign = async (
  files: File[],
  analysisData: VideoScene[] | null,
  description: string,
  language: Language,
  duration: Duration,
  count: number
): Promise<GeneratedContent> => {
  const imageParts = await Promise.all(files.map(fileToGenerativePart));
  
  // Step 1: Generate White Background Product Grid
  const productReferenceBase64 = await generateProductGrid(imageParts, description);

  // Step 2: Generate Scripts based on images + analysis
  const variants = await generateScriptAndPrompt(imageParts, analysisData, description, language, duration, count);

  return {
    productReferenceBase64,
    visualAssetBase64: null, // Generated later
    variants
  };
};

// Legacy exports...
export const regenerateVisualsOnly = async (files: File[], description: string) => { return ""; };
export const regenerateStrategyOnly = async (
  files: File[],
  analysisData: VideoScene[] | null,
  description: string,
  language: Language,
  duration: Duration,
  count: number
) => {
  const imageParts = await Promise.all(files.map(fileToGenerativePart));
  return generateScriptAndPrompt(imageParts, analysisData, description, language, duration, count);
};
