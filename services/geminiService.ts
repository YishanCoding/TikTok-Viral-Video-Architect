import { GoogleGenAI, Type, Schema } from "@google/genai";
import { Language, Duration, ScriptRow, GeneratedContent, ScriptVariant } from "../types";

// Helper to convert file to base64
export const fileToGenerativePart = async (file: File): Promise<{ inlineData: { mimeType: string; data: string } }> => {
  return new Promise((resolve, reject) => {
    const reader = new FileReader();
    reader.onloadend = () => {
      const base64String = reader.result as string;
      const base64Data = base64String.split(',')[1];
      resolve({
        inlineData: {
          mimeType: file.type,
          data: base64Data
        }
      });
    };
    reader.onerror = reject;
    reader.readAsDataURL(file);
  });
};

/**
 * Step 1: Generate the "9-grid" reference image 
 */
export const generateReferenceImage = async (imageParts: any[], description: string): Promise<string> => {
  try {
    const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });

    const prompt = `
      Create a "9-grid reference sheet" (3x3 grid layout) for this product based on the provided images.
      The image should contain 9 distinct panels showing the product from different angles, 
      close-ups of key details described as "${description}", and lifestyle usage context.
      Style: High-quality iPhone photography, authentic UGC (User Generated Content) feel, bright lighting.
      Do not add text overlays.
    `;

    const response = await ai.models.generateContent({
      model: 'gemini-3-pro-image-preview',
      contents: {
        parts: [
          ...imageParts,
          { text: prompt }
        ]
      },
      config: {
        imageConfig: {
          aspectRatio: "9:16", 
          imageSize: "4K"      
        }
      }
    });

    for (const part of response.candidates?.[0]?.content?.parts || []) {
      if (part.inlineData) {
        return part.inlineData.data;
      }
    }
    throw new Error("No image generated by the model.");
  } catch (error) {
    console.error("Error generating reference image:", error);
    throw error;
  }
};

/**
 * Step 2: Generate the Script and Sora Prompt (Supports Multiple Variants)
 */
export const generateScriptAndPrompt = async (
  imageParts: any[],
  videoPart: any | null,
  description: string, 
  language: Language, 
  duration: Duration,
  count: number = 1
): Promise<ScriptVariant[]> => {
  try {
    const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });

    const durationSec = duration === Duration.SHORT ? 15 : 25;
    
    const systemInstruction = `
      You are a world-class TikTok E-commerce Short Video Expert.
      Your goal is to create ${count} DISTINCT high-converting UGC (User Generated Content) video scripts and video generation prompts.
      
      CONSTRAINTS:
      1. Language: STRICTLY output the script text in ${language}.
      2. Duration: Each script must fit exactly into ${durationSec} seconds.
      3. Tone: Authentic, native TikTok creator style. NO "TV commercial" vibes.
      4. Diversity: Each variant must have a different "Hook" angle (e.g., ASMR, Problem/Solution, Unexpected Twist).
    `;

    let userPrompt = `
      Product Description: ${description}
      
      Task 1: Create ${count} different TikTok Scripts in a table format.
      Task 2: Create a corresponding Sora-2 Prompt for each script (in English).
      
      IMPORTANT for Sora Prompt:
      - This prompt will be used in an "Image-to-Video" workflow using the generated 9-grid image.
      - Incorporate the script's visual progression into the prompt.
      - Style: Handheld, iPhone quality, TikTok aesthetic.
    `;

    if (videoPart) {
      userPrompt += `\n\nReference Video Analysis: I have attached a best-selling viral video. Deconstruct its success formula and apply it to the scripts.`;
    }

    // Schema for an Object containing an Array of Variants
    const responseSchema: Schema = {
      type: Type.OBJECT,
      properties: {
        variants: {
          type: Type.ARRAY,
          items: {
            type: Type.OBJECT,
            properties: {
              name: { type: Type.STRING, description: "A short catchy name for this angle (e.g., 'The ASMR Approach')" },
              script: {
                type: Type.ARRAY,
                items: {
                  type: Type.OBJECT,
                  properties: {
                    timeframe: { type: Type.STRING, description: "e.g., '00-03s'" },
                    visual: { type: Type.STRING, description: "Visual description" },
                    audio: { type: Type.STRING, description: `Spoken audio in ${language}` },
                    style: { type: Type.STRING, description: "e.g., UGC, POV" }
                  },
                  required: ["timeframe", "visual", "audio", "style"]
                }
              },
              soraPrompt: {
                type: Type.STRING,
                description: "English prompt for Sora-2."
              }
            },
            required: ["name", "script", "soraPrompt"]
          }
        }
      },
      required: ["variants"]
    };

    const parts = [...imageParts];
    if (videoPart) {
      parts.push(videoPart);
    }
    parts.push({ text: userPrompt });

    const response = await ai.models.generateContent({
      model: 'gemini-3-flash-preview',
      contents: {
        parts: parts
      },
      config: {
        systemInstruction: systemInstruction,
        responseMimeType: "application/json",
        responseSchema: responseSchema
      }
    });

    const jsonText = response.text;
    if (!jsonText) throw new Error("No text response from script model.");
    
    const parsed = JSON.parse(jsonText);
    
    // Add IDs to variants
    return parsed.variants.map((v: any) => ({
      ...v,
      id: crypto.randomUUID()
    }));

  } catch (error) {
    console.error("Error generating scripts:", error);
    throw error;
  }
};

/**
 * Orchestrator function (Initial Generation)
 */
export const generateCampaign = async (
  files: File[],
  videoFile: File | null,
  description: string,
  language: Language,
  duration: Duration,
  count: number
): Promise<GeneratedContent> => {
  const imageParts = await Promise.all(files.map(fileToGenerativePart));
  
  let videoPart = null;
  if (videoFile) {
    videoPart = await fileToGenerativePart(videoFile);
  }

  const [visualAssetBase64, variants] = await Promise.all([
    generateReferenceImage(imageParts, description),
    generateScriptAndPrompt(imageParts, videoPart, description, language, duration, count)
  ]);

  return {
    visualAssetBase64,
    variants
  };
};

/**
 * Regenerate ONLY Visuals
 */
export const regenerateVisualsOnly = async (
  files: File[],
  description: string
): Promise<string> => {
  const imageParts = await Promise.all(files.map(fileToGenerativePart));
  return generateReferenceImage(imageParts, description);
};

/**
 * Regenerate ONLY Strategy (Scripts)
 */
export const regenerateStrategyOnly = async (
  files: File[],
  videoFile: File | null,
  description: string,
  language: Language,
  duration: Duration,
  count: number
): Promise<ScriptVariant[]> => {
  const imageParts = await Promise.all(files.map(fileToGenerativePart));
  let videoPart = null;
  if (videoFile) {
    videoPart = await fileToGenerativePart(videoFile);
  }
  return generateScriptAndPrompt(imageParts, videoPart, description, language, duration, count);
};
