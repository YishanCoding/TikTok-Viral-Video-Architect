import { GoogleGenAI, Type, Schema } from "@google/genai";
import { Language, Duration, ScriptRow, GeneratedContent, ScriptVariant, VideoScene } from "../types";

// --- Helper: Convert File to Base64 (Gemini Part) ---
export const fileToGenerativePart = async (file: File): Promise<{ inlineData: { mimeType: string; data: string } }> => {
  return new Promise((resolve, reject) => {
    const reader = new FileReader();
    reader.onloadend = () => {
      const base64String = reader.result as string;
      const base64Data = base64String.split(',')[1];
      resolve({
        inlineData: {
          mimeType: file.type,
          data: base64Data
        }
      });
    };
    reader.onerror = reject;
    reader.readAsDataURL(file);
  });
};

// --- Helper: Capture Video Frames ---
export const captureVideoFrames = async (videoFile: File, timestamps: { start: number }[]): Promise<string[]> => {
  return new Promise((resolve, reject) => {
    const video = document.createElement('video');
    video.preload = 'metadata';
    video.src = URL.createObjectURL(videoFile);
    video.muted = true;
    video.playsInline = true;

    const canvas = document.createElement('canvas');
    const ctx = canvas.getContext('2d');
    const screenshots: string[] = [];
    let currentIndex = 0;

    video.onloadedmetadata = () => {
      canvas.width = video.videoWidth / 2; // Reduce resolution for performance
      canvas.height = video.videoHeight / 2;
      
      const captureNext = () => {
        if (currentIndex >= timestamps.length) {
          URL.revokeObjectURL(video.src);
          resolve(screenshots);
          return;
        }

        video.currentTime = timestamps[currentIndex].start;
      };

      video.onseeked = () => {
        if (ctx) {
            ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
            screenshots.push(canvas.toDataURL('image/jpeg', 0.7)); // Compress
        }
        currentIndex++;
        captureNext();
      };

      captureNext(); // Start loop
    };

    video.onerror = () => {
      reject(new Error("Failed to load video for frame capture"));
    };
  });
};

// --- API: Generate White Background Product Grid (Step 1) ---
export const generateProductGrid = async (imageParts: any[], description: string): Promise<string> => {
  try {
    const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });

    const prompt = `
      Create a "9-grid reference sheet" (3x3 grid layout) for this product on a pure WHITE BACKGROUND.
      The product is described as: "${description}".
      The grid should contain 9 distinct panels showing:
      - Front view
      - Back view
      - Side profiles
      - Top/down view
      - Close-ups of material textures
      - Detail shots of features
      Style: Professional E-commerce Product Photography, Studio Lighting, Clean, High Resolution. 
      No text overlays, No props, just the product.
    `;

    const response = await ai.models.generateContent({
      model: 'gemini-3-pro-image-preview', // High quality for reference
      contents: {
        parts: [
          ...imageParts,
          { text: prompt }
        ]
      },
      config: {
        imageConfig: {
          aspectRatio: "9:16",
          imageSize: "4K"
        }
      }
    });

    for (const part of response.candidates?.[0]?.content?.parts || []) {
      if (part.inlineData) {
        return part.inlineData.data;
      }
    }
    throw new Error("No image generated by the model.");
  } catch (error) {
    console.error("Error generating product grid:", error);
    throw error;
  }
};

// --- API: Analyze Video Content ---
export const analyzeVideoContent = async (videoFile: File): Promise<Omit<VideoScene, 'screenshot'>[]> => {
  try {
    const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });
    const videoPart = await fileToGenerativePart(videoFile);

    // Added stronger instructions for audio transcription
    const prompt = `
      Analyze this TikTok video in detail. Focus on both VISUALS and AUDIO.
      
      CRITICAL INSTRUCTION: You MUST LISTEN to the audio track and provide a word-for-word transcript.
      
      Break it down into scenes/shots. For each scene, provide:
      1. start_time (seconds, float)
      2. end_time (seconds, float)
      3. category (Choose from: 'Product Info', 'Usage', 'Benefits', 'Hook', 'CTA', 'Other')
      4. description (Visual description)
      5. transcript_original (Exact spoken words in original language. If silence/music, say "[Music]")
      6. transcript_translation (Translate spoken words to Simplified Chinese)

      Return strictly JSON.
    `;

    const responseSchema: Schema = {
      type: Type.OBJECT,
      properties: {
        scenes: {
          type: Type.ARRAY,
          items: {
            type: Type.OBJECT,
            properties: {
              start_time: { type: Type.NUMBER },
              end_time: { type: Type.NUMBER },
              category: { type: Type.STRING, enum: ['Product Info', 'Usage', 'Benefits', 'Hook', 'CTA', 'Other'] },
              description: { type: Type.STRING },
              transcript_original: { type: Type.STRING },
              transcript_translation: { type: Type.STRING },
            },
            required: ["start_time", "end_time", "category", "description", "transcript_original", "transcript_translation"]
          }
        }
      },
      required: ["scenes"]
    };

    const response = await ai.models.generateContent({
      model: 'gemini-3-flash-preview',
      contents: { parts: [videoPart, { text: prompt }] },
      config: {
        systemInstruction: "You are a video analysis expert capable of hearing audio and seeing visuals.",
        responseMimeType: "application/json",
        responseSchema: responseSchema
      }
    });

    const data = JSON.parse(response.text || "{}");
    
    // Format times to "MM:SS" string for UI
    const formatTime = (s: number) => {
        const mins = Math.floor(s / 60);
        const secs = Math.floor(s % 60);
        return `${mins.toString().padStart(2, '0')}:${secs.toString().padStart(2, '0')}`;
    };

    return data.scenes.map((s: any) => ({
      startTime: formatTime(s.start_time),
      endTime: formatTime(s.end_time),
      category: s.category,
      description: s.description,
      transcriptOriginal: s.transcript_original,
      transcriptTranslation: s.transcript_translation,
      rawStartTime: s.start_time // Keep raw for seeking
    }));

  } catch (error) {
    console.error("Analysis failed:", error);
    throw error;
  }
};

// --- API: Generate Single Scene Image (Nano Banana) with Reference ---
export const generateSceneImage = async (visualDescription: string, referenceImageBase64: string | null): Promise<string> => {
    const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });
    
    const parts: any[] = [];
    if (referenceImageBase64) {
      parts.push({
        inlineData: {
          mimeType: 'image/jpeg',
          data: referenceImageBase64
        }
      });
    }

    const prompt = `
      Generate a single photorealistic 9:16 vertical TikTok video frame.
      
      Scene Description: ${visualDescription}
      
      IMPORTANT CONSTRAINT: 
      The product in the generated image MUST LOOK EXACTLY like the product in the provided Reference Image.
      Use the Reference Image as the ground truth for product appearance (color, shape, logo, material).
      Style: iPhone UGC, authentic, high quality.
    `;

    parts.push({ text: prompt });

    const response = await ai.models.generateContent({
        model: 'gemini-2.5-flash-image', // Nano Banana
        contents: { parts: parts },
        config: {}
    });

    for (const part of response.candidates?.[0]?.content?.parts || []) {
        if (part.inlineData) {
            return part.inlineData.data;
        }
    }
    throw new Error("No image generated");
};

// --- API: Regenerate Single Script Row ---
export const regenerateScriptRow = async (
    originalRow: ScriptRow, 
    context: string, 
    language: string
): Promise<ScriptRow> => {
    const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });
    const prompt = `
        Regenerate this specific script row to be more engaging.
        Context of video: ${context}
        
        Current Row:
        Time: ${originalRow.timeframe}
        Visual: ${originalRow.visual}
        Audio: ${originalRow.audio}
        
        Task: Rewrite the Visual and Audio (in ${language}). Keep timeframe same.
    `;

    const responseSchema: Schema = {
         type: Type.OBJECT,
         properties: {
             visual: { type: Type.STRING },
             audio: { type: Type.STRING },
             style: { type: Type.STRING }
         }
    };

    const response = await ai.models.generateContent({
        model: 'gemini-3-flash-preview',
        contents: { parts: [{ text: prompt }] },
        config: { responseMimeType: "application/json", responseSchema }
    });

    const data = JSON.parse(response.text || "{}");
    return {
        ...originalRow,
        visual: data.visual,
        audio: data.audio,
        style: data.style
    };
};

/**
 * Step 2: Generate the Script and Sora Prompt (Updated to use Analysis)
 */
export const generateScriptAndPrompt = async (
  imageParts: any[],
  analysisData: VideoScene[] | null,
  description: string, 
  language: Language, 
  duration: Duration,
  count: number = 1
): Promise<ScriptVariant[]> => {
  try {
    const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });
    const durationSec = duration === Duration.SHORT ? 15 : 25;
    
    // Construct Analysis Context String
    let analysisContext = "";
    if (analysisData) {
        analysisContext = `
        REFERENCE VIDEO ANALYSIS (Use this structure as a proven viral template):
        ${analysisData.map(s => `[${s.startTime}-${s.endTime}] (${s.category}):
           Visual: ${s.description}
           Audio: ${s.transcriptOriginal}
        `).join('\n')}
        `;
    }

    const systemInstruction = `
      You are a world-class TikTok E-commerce Short Video Expert.
      Your goal is to create ${count} DISTINCT high-converting UGC video scripts.
      
      CONSTRAINTS:
      1. Language: STRICTLY output the script audio in ${language}.
      2. Duration: Each script must fit exactly into ${durationSec} seconds.
      3. Tone: Authentic, native TikTok creator style.
      ${analysisData ? "4. Structure: STRICTLY FOLLOW the pacing and category flow of the Reference Video Analysis provided." : ""}
    `;

    let userPrompt = `
      Product Description: ${description}
      ${analysisContext}
      
      Task 1: Create ${count} different TikTok Scripts.
      Task 2: Create a corresponding Sora-2 Prompt for each script.
    `;

    const responseSchema: Schema = {
      type: Type.OBJECT,
      properties: {
        variants: {
          type: Type.ARRAY,
          items: {
            type: Type.OBJECT,
            properties: {
              name: { type: Type.STRING },
              script: {
                type: Type.ARRAY,
                items: {
                  type: Type.OBJECT,
                  properties: {
                    timeframe: { type: Type.STRING },
                    visual: { type: Type.STRING },
                    audio: { type: Type.STRING },
                    style: { type: Type.STRING }
                  },
                  required: ["timeframe", "visual", "audio", "style"]
                }
              },
              soraPrompt: { type: Type.STRING }
            },
            required: ["name", "script", "soraPrompt"]
          }
        }
      },
      required: ["variants"]
    };

    const parts = [...imageParts, { text: userPrompt }];

    const response = await ai.models.generateContent({
      model: 'gemini-3-flash-preview',
      contents: { parts },
      config: {
        systemInstruction,
        responseMimeType: "application/json",
        responseSchema
      }
    });

    const jsonText = response.text;
    const parsed = JSON.parse(jsonText);
    
    return parsed.variants.map((v: any) => ({
      ...v,
      id: crypto.randomUUID(),
      script: v.script.map((s: any) => ({ ...s, id: crypto.randomUUID() })) // Ensure IDs for rows
    }));

  } catch (error) {
    console.error("Error generating scripts:", error);
    throw error;
  }
};

/**
 * Orchestrator function 
 */
export const generateCampaign = async (
  files: File[],
  analysisData: VideoScene[] | null,
  description: string,
  language: Language,
  duration: Duration,
  count: number
): Promise<GeneratedContent> => {
  const imageParts = await Promise.all(files.map(fileToGenerativePart));
  
  // Step 1: Generate White Background Product Grid
  const productReferenceBase64 = await generateProductGrid(imageParts, description);

  // Step 2: Generate Scripts based on images + analysis
  const variants = await generateScriptAndPrompt(imageParts, analysisData, description, language, duration, count);

  return {
    productReferenceBase64,
    visualAssetBase64: null, // Generated later
    variants
  };
};

// Legacy exports...
export const regenerateVisualsOnly = async (files: File[], description: string) => { return ""; };
export const regenerateStrategyOnly = async (
  files: File[],
  analysisData: VideoScene[] | null,
  description: string,
  language: Language,
  duration: Duration,
  count: number
) => {
  const imageParts = await Promise.all(files.map(fileToGenerativePart));
  return generateScriptAndPrompt(imageParts, analysisData, description, language, duration, count);
};